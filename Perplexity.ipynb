{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1176fff0-1496-49f6-8d37-ed3d4b7b5c80",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c18f32-3641-4fd0-880c-3650300313d6",
   "metadata": {},
   "source": [
    "## Blaise Swartwood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3badbd6-b4e7-4d62-a7d1-cd269e430902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swartwba/GPTModel/venv_tinystories/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117cbad3-2efb-46b5-a2fb-1ad7eb0c0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL_PATH = \"./models/tinystories_gpt_1layer/final_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6afa72a-6c40-4e64-b36b-bb3a5c3516d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Once upon a time, in a land far away,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45085d60-f428-436a-9399-692109267b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading tokenizer and model from: ./models/tinystories_gpt_1layer/final_model\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"Loading tokenizer and model from: {DEFAULT_MODEL_PATH}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL_PATH)\n",
    "    if tokenizer.pad_token is None:\n",
    "         tokenizer.pad_token = tokenizer.eos_token\n",
    "         print(f\"Set pad_token to eos_token ({tokenizer.pad_token}) after loading.\")\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(DEFAULT_MODEL_PATH)\n",
    "    model.to(device) \n",
    "    model.eval()    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    print(\"Ensure the path is correct and contains the necessary files \")\n",
    "    print(\"(pytorch_model.bin, config.json, tokenizer.json, etc.)\")\n",
    "    print(\"These should be saved by train_gpt.py in the 'final_model' subdirectory.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbb560a-1fcc-444d-8f00-0076e3ebc80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(PROMPT, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b4e40d6-e778-4287-8e80-0f9e7407b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(PROMPT, model, tokenizer, device):\n",
    "    inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # Forward pass (with labels shifted by one position)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Compute perplexity\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Extract probabilities of actual next tokens\n",
    "    # Shift tokens and logits for next-token prediction\n",
    "    shifted_logits = logits[:, :-1, :]\n",
    "    shifted_labels = input_ids[:, 1:]\n",
    "\n",
    "    # Probabilities for actual tokens\n",
    "    shifted_probs = F.softmax(shifted_logits, dim=-1)\n",
    "    actual_token_probs = torch.gather(\n",
    "        shifted_probs, dim=-1, index=shifted_labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    return perplexity.item(), actual_token_probs.cpu().numpy(), input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98f118d6-d001-4274-8045-ebe8c476e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swartwba/GPTModel/venv_tinystories/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Greedy:\n",
      " Once upon a time, in a land far away, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, scary monster. The monster was very scary and Lily was scared.\n",
      "\n",
      "ğŸ”¹ Sample:\n",
      " Once upon a time, in a land far away, there was a little boy named Tom. Tom loved to explore and see all the things he could find. One day, he found a big box and went inside the hunt.\n",
      "\n",
      "Tom was\n",
      "\n",
      "ğŸ”¹ Top-k:\n",
      " Once upon a time, in a land far away, there lived a little girl named Lily. One day, she went to a big mountain with her mom. While they were walking, Lily saw a scary bug who was scared. She didn't like\n",
      "\n",
      "ğŸ”¹ Top-p:\n",
      " Once upon a time, in a land far away, there was a nice girl named Lily. She loved to play with her toys, but one day she accidentally stepped on the walls. Her friend was upset because she was playing with her toys.\n",
      "\n",
      "\n",
      "ğŸ”¹ Beam Search:\n",
      " Once upon a time, in a land far away, there was a little boy named Timmy. Timmy loved to play with his toys all day long. One day, Timmy's mom asked him to clean up his toys. Timmy didn\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "temperature = 1.0\n",
    "num_return_sequences = 1\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Generation loop\n",
    "with torch.no_grad():\n",
    "    # ğŸ”¹ Greedy decoding\n",
    "    greedy_output = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=1.0,      # Doesn't matter for greedy (no sampling)\n",
    "        top_k=0,              # Disable top-k\n",
    "        top_p=1.0,            # Disable nucleus sampling\n",
    "        do_sample=False,      # â† Greedy decoding\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    # Normal Sampling\n",
    "    sample_output = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=1.0,\n",
    "        top_k=0,             # Disable top-k sampling\n",
    "        top_p=1.0,            # Disable nucleus\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    # ğŸ”¹ Top-k sampling\n",
    "    topk_output = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=1.0,\n",
    "        top_k=50,             # Enable top-k sampling\n",
    "        top_p=1.0,            # Disable nucleus\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    # ğŸ”¹ Top-p (nucleus) sampling\n",
    "    topp_output = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=1.0,\n",
    "        top_k=0,              # Disable top-k\n",
    "        top_p=0.9,            # Enable top-p sampling\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    beam_output = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        num_beams=5,            # Use beam width of 5 (can be tuned)\n",
    "        do_sample=False,        # Deterministic beam search\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=pad_token_id,\n",
    "        early_stopping=True     # Optional: stop when all beams reach EOS\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "print(\"ğŸ”¹ Greedy:\\n\", tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
    "print(\"ğŸ”¹ Sample:\\n\", tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "print(\"\\nğŸ”¹ Top-k:\\n\", tokenizer.decode(topk_output[0], skip_special_tokens=True))\n",
    "print(\"\\nğŸ”¹ Top-p:\\n\", tokenizer.decode(topp_output[0], skip_special_tokens=True))\n",
    "print(\"\\nğŸ”¹ Beam Search:\\n\", tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1658127-5524-4b6e-ab37-d5343c94051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_print(generated_output, strategy_name):\n",
    "    text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    perplexity, token_probs, input_ids = compute_perplexity(text, model, tokenizer, device=\"cuda\")\n",
    "    tokens = input_ids[0].tolist()\n",
    "    decoded_tokens = [tokenizer.decode([tid]) for tid in tokens]\n",
    "\n",
    "    print(f\"\\nğŸ”¹ {strategy_name} Sampling\")\n",
    "    print(\"=\" * (len(strategy_name) + 12))\n",
    "    print(f\"Generated Text:\\n{text}\\n\")\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n",
    "    print(\"Next-token prediction probabilities:\")\n",
    "    for i, prob in enumerate(token_probs[0]):\n",
    "        prev_token = decoded_tokens[i]\n",
    "        actual_next_token = decoded_tokens[i + 1]\n",
    "        print(f\"After '{prev_token}' â†’ '{actual_next_token}': {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f09df7-7ad1-4628-830b-9a0b2f2ceeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¹ Greedy Sampling\n",
      "==================\n",
      "Generated Text:\n",
      "Once upon a time, in a land far away, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, scary monster. The monster was very scary and Lily was scared.\n",
      "\n",
      "\n",
      "Perplexity: 2.24\n",
      "Next-token prediction probabilities:\n",
      "After 'Once' â†’ ' upon': 0.8850\n",
      "After ' upon' â†’ ' a': 0.9836\n",
      "After ' a' â†’ ' time': 0.9991\n",
      "After ' time' â†’ ',': 0.7619\n",
      "After ',' â†’ ' in': 0.0043\n",
      "After ' in' â†’ ' a': 0.9557\n",
      "After ' a' â†’ ' land': 0.0046\n",
      "After ' land' â†’ ' far': 0.1259\n",
      "After ' far' â†’ ' away': 0.7399\n",
      "After ' away' â†’ ',': 0.0409\n",
      "After ',' â†’ ' there': 0.9376\n",
      "After ' there' â†’ ' was': 0.6122\n",
      "After ' was' â†’ ' a': 0.9760\n",
      "After ' a' â†’ ' little': 0.4792\n",
      "After ' little' â†’ ' girl': 0.7101\n",
      "After ' girl' â†’ ' named': 0.8823\n",
      "After ' named' â†’ ' Lily': 0.9388\n",
      "After ' Lily' â†’ '.': 0.9670\n",
      "After '.' â†’ ' She': 0.9013\n",
      "After ' She' â†’ ' loved': 0.8367\n",
      "After ' loved' â†’ ' to': 0.9095\n",
      "After ' to' â†’ ' play': 0.7249\n",
      "After ' play' â†’ ' outside': 0.4339\n",
      "After ' outside' â†’ ' in': 0.4937\n",
      "After ' in' â†’ ' the': 0.8909\n",
      "After ' the' â†’ ' sunshine': 0.3383\n",
      "After ' sunshine' â†’ '.': 0.6979\n",
      "After '.' â†’ ' One': 0.9747\n",
      "After ' One' â†’ ' day': 0.9897\n",
      "After ' day' â†’ ',': 0.9994\n",
      "After ',' â†’ ' she': 0.7562\n",
      "After ' she' â†’ ' saw': 0.3822\n",
      "After ' saw' â†’ ' a': 0.8956\n",
      "After ' a' â†’ ' big': 0.4090\n",
      "After ' big' â†’ ',': 0.1843\n",
      "After ',' â†’ ' scary': 0.1975\n",
      "After ' scary' â†’ ' monster': 0.3276\n",
      "After ' monster' â†’ '.': 0.2915\n",
      "After '.' â†’ ' The': 0.4345\n",
      "After ' The' â†’ ' monster': 0.9870\n",
      "After ' monster' â†’ ' was': 0.5181\n",
      "After ' was' â†’ ' very': 0.1930\n",
      "After ' very' â†’ ' scary': 0.3833\n",
      "After ' scary' â†’ ' and': 0.5972\n",
      "After ' and' â†’ ' Lily': 0.2413\n",
      "After ' Lily' â†’ ' was': 0.2724\n",
      "After ' was' â†’ ' scared': 0.7048\n",
      "After ' scared' â†’ '.': 0.6838\n",
      "After '.' â†’ '\n",
      "': 0.3760\n",
      "\n",
      "ğŸ”¹ Sample Sampling\n",
      "==================\n",
      "Generated Text:\n",
      "Once upon a time, in a land far away, there was a little boy named Tom. Tom loved to explore and see all the things he could find. One day, he found a big box and went inside the hunt.\n",
      "\n",
      "Tom was\n",
      "\n",
      "Perplexity: 4.48\n",
      "Next-token prediction probabilities:\n",
      "After 'Once' â†’ ' upon': 0.8850\n",
      "After ' upon' â†’ ' a': 0.9836\n",
      "After ' a' â†’ ' time': 0.9991\n",
      "After ' time' â†’ ',': 0.7619\n",
      "After ',' â†’ ' in': 0.0043\n",
      "After ' in' â†’ ' a': 0.9557\n",
      "After ' a' â†’ ' land': 0.0046\n",
      "After ' land' â†’ ' far': 0.1259\n",
      "After ' far' â†’ ' away': 0.7399\n",
      "After ' away' â†’ ',': 0.0409\n",
      "After ',' â†’ ' there': 0.9376\n",
      "After ' there' â†’ ' was': 0.6122\n",
      "After ' was' â†’ ' a': 0.9760\n",
      "After ' a' â†’ ' little': 0.4792\n",
      "After ' little' â†’ ' boy': 0.1997\n",
      "After ' boy' â†’ ' named': 0.8969\n",
      "After ' named' â†’ ' Tom': 0.0135\n",
      "After ' Tom' â†’ '.': 0.9847\n",
      "After '.' â†’ ' Tom': 0.9287\n",
      "After ' Tom' â†’ ' loved': 0.5134\n",
      "After ' loved' â†’ ' to': 0.9607\n",
      "After ' to' â†’ ' explore': 0.0350\n",
      "After ' explore' â†’ ' and': 0.2783\n",
      "After ' and' â†’ ' see': 0.0192\n",
      "After ' see' â†’ ' all': 0.1194\n",
      "After ' all' â†’ ' the': 0.7399\n",
      "After ' the' â†’ ' things': 0.1738\n",
      "After ' things' â†’ ' he': 0.3843\n",
      "After ' he' â†’ ' could': 0.4441\n",
      "After ' could' â†’ ' find': 0.4095\n",
      "After ' find' â†’ '.': 0.9125\n",
      "After '.' â†’ ' One': 0.6193\n",
      "After ' One' â†’ ' day': 0.9866\n",
      "After ' day' â†’ ',': 0.9753\n",
      "After ',' â†’ ' he': 0.3178\n",
      "After ' he' â†’ ' found': 0.2656\n",
      "After ' found' â†’ ' a': 0.9157\n",
      "After ' a' â†’ ' big': 0.2575\n",
      "After ' big' â†’ ' box': 0.1701\n",
      "After ' box' â†’ ' and': 0.0348\n",
      "After ' and' â†’ ' went': 0.0296\n",
      "After ' went' â†’ ' inside': 0.3343\n",
      "After ' inside' â†’ ' the': 0.0539\n",
      "After ' the' â†’ ' hunt': 0.0000\n",
      "After ' hunt' â†’ '.': 0.9183\n",
      "After '.' â†’ '\n",
      "': 0.3420\n",
      "After '\n",
      "' â†’ '\n",
      "': 0.9924\n",
      "After '\n",
      "' â†’ 'Tom': 0.6874\n",
      "After 'Tom' â†’ ' was': 0.2007\n",
      "\n",
      "ğŸ”¹ Top-k Sampling\n",
      "=================\n",
      "Generated Text:\n",
      "Once upon a time, in a land far away, there lived a little girl named Lily. One day, she went to a big mountain with her mom. While they were walking, Lily saw a scary bug who was scared. She didn't like\n",
      "\n",
      "Perplexity: 3.30\n",
      "Next-token prediction probabilities:\n",
      "After 'Once' â†’ ' upon': 0.8850\n",
      "After ' upon' â†’ ' a': 0.9836\n",
      "After ' a' â†’ ' time': 0.9991\n",
      "After ' time' â†’ ',': 0.7619\n",
      "After ',' â†’ ' in': 0.0043\n",
      "After ' in' â†’ ' a': 0.9557\n",
      "After ' a' â†’ ' land': 0.0046\n",
      "After ' land' â†’ ' far': 0.1259\n",
      "After ' far' â†’ ' away': 0.7399\n",
      "After ' away' â†’ ',': 0.0409\n",
      "After ',' â†’ ' there': 0.9376\n",
      "After ' there' â†’ ' lived': 0.3566\n",
      "After ' lived' â†’ ' a': 0.9638\n",
      "After ' a' â†’ ' little': 0.4231\n",
      "After ' little' â†’ ' girl': 0.6976\n",
      "After ' girl' â†’ ' named': 0.8474\n",
      "After ' named' â†’ ' Lily': 0.8880\n",
      "After ' Lily' â†’ '.': 0.9795\n",
      "After '.' â†’ ' One': 0.1163\n",
      "After ' One' â†’ ' day': 0.9714\n",
      "After ' day' â†’ ',': 0.9994\n",
      "After ',' â†’ ' she': 0.6503\n",
      "After ' she' â†’ ' went': 0.5201\n",
      "After ' went' â†’ ' to': 0.5998\n",
      "After ' to' â†’ ' a': 0.2890\n",
      "After ' a' â†’ ' big': 0.2159\n",
      "After ' big' â†’ ' mountain': 0.0273\n",
      "After ' mountain' â†’ ' with': 0.4096\n",
      "After ' with' â†’ ' her': 0.9514\n",
      "After ' her' â†’ ' mom': 0.8179\n",
      "After ' mom' â†’ '.': 0.1747\n",
      "After '.' â†’ ' While': 0.0467\n",
      "After ' While' â†’ ' they': 0.6725\n",
      "After ' they' â†’ ' were': 0.9675\n",
      "After ' were' â†’ ' walking': 0.5370\n",
      "After ' walking' â†’ ',': 0.9266\n",
      "After ',' â†’ ' Lily': 0.7859\n",
      "After ' Lily' â†’ ' saw': 0.8204\n",
      "After ' saw' â†’ ' a': 0.9471\n",
      "After ' a' â†’ ' scary': 0.0097\n",
      "After ' scary' â†’ ' bug': 0.0263\n",
      "After ' bug' â†’ ' who': 0.0215\n",
      "After ' who' â†’ ' was': 0.5220\n",
      "After ' was' â†’ ' scared': 0.1025\n",
      "After ' scared' â†’ '.': 0.6030\n",
      "After '.' â†’ ' She': 0.3621\n",
      "After ' She' â†’ ' didn': 0.1319\n",
      "After ' didn' â†’ ''t': 0.9987\n",
      "After ''t' â†’ ' like': 0.0668\n",
      "\n",
      "ğŸ”¹ Top-p Sampling\n",
      "=================\n",
      "Generated Text:\n",
      "Once upon a time, in a land far away, there was a nice girl named Lily. She loved to play with her toys, but one day she accidentally stepped on the walls. Her friend was upset because she was playing with her toys.\n",
      "\n",
      "\n",
      "Perplexity: 3.86\n",
      "Next-token prediction probabilities:\n",
      "After 'Once' â†’ ' upon': 0.8850\n",
      "After ' upon' â†’ ' a': 0.9836\n",
      "After ' a' â†’ ' time': 0.9991\n",
      "After ' time' â†’ ',': 0.7619\n",
      "After ',' â†’ ' in': 0.0043\n",
      "After ' in' â†’ ' a': 0.9557\n",
      "After ' a' â†’ ' land': 0.0046\n",
      "After ' land' â†’ ' far': 0.1259\n",
      "After ' far' â†’ ' away': 0.7399\n",
      "After ' away' â†’ ',': 0.0409\n",
      "After ',' â†’ ' there': 0.9376\n",
      "After ' there' â†’ ' was': 0.6122\n",
      "After ' was' â†’ ' a': 0.9760\n",
      "After ' a' â†’ ' nice': 0.0075\n",
      "After ' nice' â†’ ' girl': 0.1055\n",
      "After ' girl' â†’ ' named': 0.7952\n",
      "After ' named' â†’ ' Lily': 0.8776\n",
      "After ' Lily' â†’ '.': 0.9651\n",
      "After '.' â†’ ' She': 0.8676\n",
      "After ' She' â†’ ' loved': 0.8218\n",
      "After ' loved' â†’ ' to': 0.9156\n",
      "After ' to' â†’ ' play': 0.6746\n",
      "After ' play' â†’ ' with': 0.3710\n",
      "After ' with' â†’ ' her': 0.9757\n",
      "After ' her' â†’ ' toys': 0.7281\n",
      "After ' toys' â†’ ',': 0.2168\n",
      "After ',' â†’ ' but': 0.4601\n",
      "After ' but' â†’ ' one': 0.1852\n",
      "After ' one' â†’ ' day': 0.9691\n",
      "After ' day' â†’ ' she': 0.7864\n",
      "After ' she' â†’ ' accidentally': 0.2133\n",
      "After ' accidentally' â†’ ' stepped': 0.0475\n",
      "After ' stepped' â†’ ' on': 0.7619\n",
      "After ' on' â†’ ' the': 0.1349\n",
      "After ' the' â†’ ' walls': 0.0025\n",
      "After ' walls' â†’ '.': 0.6186\n",
      "After '.' â†’ ' Her': 0.2531\n",
      "After ' Her' â†’ ' friend': 0.0088\n",
      "After ' friend' â†’ ' was': 0.1365\n",
      "After ' was' â†’ ' upset': 0.1415\n",
      "After ' upset' â†’ ' because': 0.2929\n",
      "After ' because' â†’ ' she': 0.6950\n",
      "After ' she' â†’ ' was': 0.0658\n",
      "After ' was' â†’ ' playing': 0.0168\n",
      "After ' playing' â†’ ' with': 0.7632\n",
      "After ' with' â†’ ' her': 0.8345\n",
      "After ' her' â†’ ' toys': 0.5202\n",
      "After ' toys' â†’ '.': 0.3307\n",
      "After '.' â†’ '\n",
      "': 0.4772\n",
      "\n",
      "ğŸ”¹ Beam Search Sampling\n",
      "=======================\n",
      "Generated Text:\n",
      "Once upon a time, in a land far away, there was a little boy named Timmy. Timmy loved to play with his toys all day long. One day, Timmy's mom asked him to clean up his toys. Timmy didn\n",
      "\n",
      "Perplexity: 1.79\n",
      "Next-token prediction probabilities:\n",
      "After 'Once' â†’ ' upon': 0.8850\n",
      "After ' upon' â†’ ' a': 0.9836\n",
      "After ' a' â†’ ' time': 0.9991\n",
      "After ' time' â†’ ',': 0.7619\n",
      "After ',' â†’ ' in': 0.0043\n",
      "After ' in' â†’ ' a': 0.9557\n",
      "After ' a' â†’ ' land': 0.0046\n",
      "After ' land' â†’ ' far': 0.1259\n",
      "After ' far' â†’ ' away': 0.7399\n",
      "After ' away' â†’ ',': 0.0409\n",
      "After ',' â†’ ' there': 0.9376\n",
      "After ' there' â†’ ' was': 0.6122\n",
      "After ' was' â†’ ' a': 0.9760\n",
      "After ' a' â†’ ' little': 0.4792\n",
      "After ' little' â†’ ' boy': 0.1997\n",
      "After ' boy' â†’ ' named': 0.8969\n",
      "After ' named' â†’ ' Tim': 0.9369\n",
      "After ' Tim' â†’ 'my': 0.6224\n",
      "After 'my' â†’ '.': 0.9802\n",
      "After '.' â†’ ' Tim': 0.9644\n",
      "After ' Tim' â†’ 'my': 0.9864\n",
      "After 'my' â†’ ' loved': 0.8845\n",
      "After ' loved' â†’ ' to': 0.9037\n",
      "After ' to' â†’ ' play': 0.7984\n",
      "After ' play' â†’ ' with': 0.5633\n",
      "After ' with' â†’ ' his': 0.9630\n",
      "After ' his' â†’ ' toys': 0.7564\n",
      "After ' toys' â†’ ' all': 0.2293\n",
      "After ' all' â†’ ' day': 0.9745\n",
      "After ' day' â†’ ' long': 0.9255\n",
      "After ' long' â†’ '.': 0.9672\n",
      "After '.' â†’ ' One': 0.7462\n",
      "After ' One' â†’ ' day': 0.9843\n",
      "After ' day' â†’ ',': 0.9990\n",
      "After ',' â†’ ' Tim': 0.8631\n",
      "After ' Tim' â†’ 'my': 0.9953\n",
      "After 'my' â†’ ''s': 0.8338\n",
      "After ''s' â†’ ' mom': 0.8102\n",
      "After ' mom' â†’ ' asked': 0.2849\n",
      "After ' asked' â†’ ' him': 0.9934\n",
      "After ' him' â†’ ' to': 0.8731\n",
      "After ' to' â†’ ' clean': 0.5518\n",
      "After ' clean' â†’ ' up': 0.8097\n",
      "After ' up' â†’ ' his': 0.8016\n",
      "After ' his' â†’ ' toys': 0.8465\n",
      "After ' toys' â†’ '.': 0.4283\n",
      "After '.' â†’ ' Tim': 0.6612\n",
      "After ' Tim' â†’ 'my': 0.9989\n",
      "After 'my' â†’ ' didn': 0.6720\n"
     ]
    }
   ],
   "source": [
    "# Run for each\n",
    "evaluate_and_print(greedy_output, \"Greedy\")\n",
    "evaluate_and_print(sample_output, \"Sample\")\n",
    "evaluate_and_print(topk_output, \"Top-k\")\n",
    "evaluate_and_print(topp_output, \"Top-p\")\n",
    "evaluate_and_print(beam_output, \"Beam Search\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT1Layer",
   "language": "python",
   "name": "venv_tinystories"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
