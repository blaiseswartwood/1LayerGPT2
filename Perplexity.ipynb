{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1176fff0-1496-49f6-8d37-ed3d4b7b5c80",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c18f32-3641-4fd0-880c-3650300313d6",
   "metadata": {},
   "source": [
    "## Blaise Swartwood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3badbd6-b4e7-4d62-a7d1-cd269e430902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swartwba/GPTModel/venv_tinystories/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "117cbad3-2efb-46b5-a2fb-1ad7eb0c0ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_MODEL_PATH = \"./models/tinystories_gpt_1layer/final_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6afa72a-6c40-4e64-b36b-bb3a5c3516d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"Once upon a time, in a land far away,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45085d60-f428-436a-9399-692109267b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading tokenizer and model from: ./models/tinystories_gpt_1layer/final_model\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(f\"Loading tokenizer and model from: {DEFAULT_MODEL_PATH}\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL_PATH)\n",
    "    if tokenizer.pad_token is None:\n",
    "         tokenizer.pad_token = tokenizer.eos_token\n",
    "         print(f\"Set pad_token to eos_token ({tokenizer.pad_token}) after loading.\")\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(DEFAULT_MODEL_PATH)\n",
    "    model.to(device) \n",
    "    model.eval()    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    print(\"Ensure the path is correct and contains the necessary files \")\n",
    "    print(\"(pytorch_model.bin, config.json, tokenizer.json, etc.)\")\n",
    "    print(\"These should be saved by train_gpt.py in the 'final_model' subdirectory.\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbb560a-1fcc-444d-8f00-0076e3ebc80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(PROMPT, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b4e40d6-e778-4287-8e80-0f9e7407b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(PROMPT, model, tokenizer, device):\n",
    "    inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # Forward pass (with labels shifted by one position)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Compute perplexity\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Extract probabilities of actual next tokens\n",
    "    # Shift tokens and logits for next-token prediction\n",
    "    shifted_logits = logits[:, :-1, :]\n",
    "    shifted_labels = input_ids[:, 1:]\n",
    "\n",
    "    # Probabilities for actual tokens\n",
    "    shifted_probs = F.softmax(shifted_logits, dim=-1)\n",
    "    actual_token_probs = torch.gather(\n",
    "        shifted_probs, dim=-1, index=shifted_labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    return perplexity.item(), actual_token_probs.cpu().numpy(), input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98f118d6-d001-4274-8045-ebe8c476e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swartwba/GPTModel/venv_tinystories/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Greedy:\n",
      " Once upon a time, in a land far away, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, scary monster. The monster was very scary and Lily was scared.\n",
      "\n",
      "\n",
      "üîπ Top-k:\n",
      " Once upon a time, in a land far away, there was a brave girl named Lily. She loved to explore and found what she was looking for. One day, she found a shiny coin that sparkled in the sky. She was so happy\n",
      "\n",
      "üîπ Top-p:\n",
      " Once upon a time, in a land far away, there lived a kind and beautiful purple cat. One day, the cat met a little girl named Sue. Sue wanted to help her friend, Tom. \"Let's play together!\" said Sue.\n"
     ]
    }
   ],
   "source": [
    "max_length = 50\n",
    "temperature = 1.0\n",
    "num_return_sequences = 1\n",
    "pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Generation loop\n",
    "with torch.no_grad():\n",
    "    # üîπ Greedy decoding\n",
    "    greedy_output = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=1.0,      # Doesn't matter for greedy (no sampling)\n",
    "        top_k=0,              # Disable top-k\n",
    "        top_p=1.0,            # Disable nucleus sampling\n",
    "        do_sample=False,      # ‚Üê Greedy decoding\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    # üîπ Top-k sampling\n",
    "    topk_output = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=1.0,\n",
    "        top_k=50,             # Enable top-k sampling\n",
    "        top_p=1.0,            # Disable nucleus\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "    # üîπ Top-p (nucleus) sampling\n",
    "    topp_output = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=1.0,\n",
    "        top_k=0,              # Disable top-k\n",
    "        top_p=0.9,            # Enable top-p sampling\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=pad_token_id\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "print(\"üîπ Greedy:\\n\", tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
    "print(\"\\nüîπ Top-k:\\n\", tokenizer.decode(topk_output[0], skip_special_tokens=True))\n",
    "print(\"\\nüîπ Top-p:\\n\", tokenizer.decode(topp_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1658127-5524-4b6e-ab37-d5343c94051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_print(generated_output, strategy_name):\n",
    "    text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    perplexity, token_probs, input_ids = compute_perplexity(text, model, tokenizer, device=\"cuda\")\n",
    "    tokens = input_ids[0].tolist()\n",
    "    decoded_tokens = [tokenizer.decode([tid]) for tid in tokens]\n",
    "\n",
    "    print(f\"\\nüîπ {strategy_name} Sampling\")\n",
    "    print(\"=\" * (len(strategy_name) + 12))\n",
    "    print(f\"Generated Text:\\n{text}\\n\")\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n",
    "    print(\"Next-token prediction probabilities:\")\n",
    "    for i, prob in enumerate(token_probs[0]):\n",
    "        prev_token = decoded_tokens[i]\n",
    "        actual_next_token = decoded_tokens[i + 1]\n",
    "        print(f\"After '{prev_token}' ‚Üí '{actual_next_token}': {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f09df7-7ad1-4628-830b-9a0b2f2ceeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Greedy Sampling\n",
      "==================\n",
      "Generated Text:\n",
      "Once upon a time, in a land far away, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big, scary monster. The monster was very scary and Lily was scared.\n",
      "\n",
      "\n",
      "Perplexity: 2.24\n",
      "Next-token prediction probabilities:\n",
      "After 'Once' ‚Üí ' upon': 0.8850\n",
      "After ' upon' ‚Üí ' a': 0.9836\n",
      "After ' a' ‚Üí ' time': 0.9991\n",
      "After ' time' ‚Üí ',': 0.7619\n",
      "After ',' ‚Üí ' in': 0.0043\n",
      "After ' in' ‚Üí ' a': 0.9557\n",
      "After ' a' ‚Üí ' land': 0.0046\n",
      "After ' land' ‚Üí ' far': 0.1259\n",
      "After ' far' ‚Üí ' away': 0.7399\n",
      "After ' away' ‚Üí ',': 0.0409\n",
      "After ',' ‚Üí ' there': 0.9376\n",
      "After ' there' ‚Üí ' was': 0.6122\n",
      "After ' was' ‚Üí ' a': 0.9760\n",
      "After ' a' ‚Üí ' little': 0.4792\n",
      "After ' little' ‚Üí ' girl': 0.7101\n",
      "After ' girl' ‚Üí ' named': 0.8823\n",
      "After ' named' ‚Üí ' Lily': 0.9388\n",
      "After ' Lily' ‚Üí '.': 0.9670\n",
      "After '.' ‚Üí ' She': 0.9013\n",
      "After ' She' ‚Üí ' loved': 0.8367\n",
      "After ' loved' ‚Üí ' to': 0.9095\n",
      "After ' to' ‚Üí ' play': 0.7249\n",
      "After ' play' ‚Üí ' outside': 0.4339\n",
      "After ' outside' ‚Üí ' in': 0.4937\n",
      "After ' in' ‚Üí ' the': 0.8909\n",
      "After ' the' ‚Üí ' sunshine': 0.3383\n",
      "After ' sunshine' ‚Üí '.': 0.6979\n",
      "After '.' ‚Üí ' One': 0.9747\n",
      "After ' One' ‚Üí ' day': 0.9897\n",
      "After ' day' ‚Üí ',': 0.9994\n",
      "After ',' ‚Üí ' she': 0.7562\n",
      "After ' she' ‚Üí ' saw': 0.3822\n",
      "After ' saw' ‚Üí ' a': 0.8956\n",
      "After ' a' ‚Üí ' big': 0.4090\n",
      "After ' big' ‚Üí ',': 0.1843\n",
      "After ',' ‚Üí ' scary': 0.1975\n",
      "After ' scary' ‚Üí ' monster': 0.3276\n",
      "After ' monster' ‚Üí '.': 0.2915\n",
      "After '.' ‚Üí ' The': 0.4345\n",
      "After ' The' ‚Üí ' monster': 0.9870\n",
      "After ' monster' ‚Üí ' was': 0.5181\n",
      "After ' was' ‚Üí ' very': 0.1930\n",
      "After ' very' ‚Üí ' scary': 0.3833\n",
      "After ' scary' ‚Üí ' and': 0.5972\n",
      "After ' and' ‚Üí ' Lily': 0.2413\n",
      "After ' Lily' ‚Üí ' was': 0.2724\n",
      "After ' was' ‚Üí ' scared': 0.7048\n",
      "After ' scared' ‚Üí '.': 0.6838\n",
      "After '.' ‚Üí '\n",
      "': 0.3760\n",
      "\n",
      "üîπ Top-k Sampling\n",
      "=================\n",
      "Generated Text:\n",
      "Once upon a time, in a land far away, there was a brave girl named Lily. She loved to explore and found what she was looking for. One day, she found a shiny coin that sparkled in the sky. She was so happy\n",
      "\n",
      "Perplexity: 3.58\n",
      "Next-token prediction probabilities:\n",
      "After 'Once' ‚Üí ' upon': 0.8850\n",
      "After ' upon' ‚Üí ' a': 0.9836\n",
      "After ' a' ‚Üí ' time': 0.9991\n",
      "After ' time' ‚Üí ',': 0.7619\n",
      "After ',' ‚Üí ' in': 0.0043\n",
      "After ' in' ‚Üí ' a': 0.9557\n",
      "After ' a' ‚Üí ' land': 0.0046\n",
      "After ' land' ‚Üí ' far': 0.1259\n",
      "After ' far' ‚Üí ' away': 0.7399\n",
      "After ' away' ‚Üí ',': 0.0409\n",
      "After ',' ‚Üí ' there': 0.9376\n",
      "After ' there' ‚Üí ' was': 0.6122\n",
      "After ' was' ‚Üí ' a': 0.9760\n",
      "After ' a' ‚Üí ' brave': 0.0108\n",
      "After ' brave' ‚Üí ' girl': 0.1494\n",
      "After ' girl' ‚Üí ' named': 0.6767\n",
      "After ' named' ‚Üí ' Lily': 0.8242\n",
      "After ' Lily' ‚Üí '.': 0.9752\n",
      "After '.' ‚Üí ' She': 0.9034\n",
      "After ' She' ‚Üí ' loved': 0.7978\n",
      "After ' loved' ‚Üí ' to': 0.9130\n",
      "After ' to' ‚Üí ' explore': 0.1195\n",
      "After ' explore' ‚Üí ' and': 0.3823\n",
      "After ' and' ‚Üí ' found': 0.0024\n",
      "After ' found' ‚Üí ' what': 0.0021\n",
      "After ' what' ‚Üí ' she': 0.3648\n",
      "After ' she' ‚Üí ' was': 0.2078\n",
      "After ' was' ‚Üí ' looking': 0.1082\n",
      "After ' looking' ‚Üí ' for': 0.7392\n",
      "After ' for' ‚Üí '.': 0.8256\n",
      "After '.' ‚Üí ' One': 0.8007\n",
      "After ' One' ‚Üí ' day': 0.9911\n",
      "After ' day' ‚Üí ',': 0.9978\n",
      "After ',' ‚Üí ' she': 0.8410\n",
      "After ' she' ‚Üí ' found': 0.4149\n",
      "After ' found' ‚Üí ' a': 0.9097\n",
      "After ' a' ‚Üí ' shiny': 0.1860\n",
      "After ' shiny' ‚Üí ' coin': 0.0870\n",
      "After ' coin' ‚Üí ' that': 0.0368\n",
      "After ' that' ‚Üí ' spark': 0.0973\n",
      "After ' spark' ‚Üí 'led': 0.9254\n",
      "After 'led' ‚Üí ' in': 0.9319\n",
      "After ' in' ‚Üí ' the': 0.8940\n",
      "After ' the' ‚Üí ' sky': 0.0716\n",
      "After ' sky' ‚Üí '.': 0.9684\n",
      "After '.' ‚Üí ' She': 0.4899\n",
      "After ' She' ‚Üí ' was': 0.3083\n",
      "After ' was' ‚Üí ' so': 0.8613\n",
      "After ' so' ‚Üí ' happy': 0.4014\n",
      "\n",
      "üîπ Top-p Sampling\n",
      "=================\n",
      "Generated Text:\n",
      "Once upon a time, in a land far away, there lived a kind and beautiful purple cat. One day, the cat met a little girl named Sue. Sue wanted to help her friend, Tom. \"Let's play together!\" said Sue.\n",
      "\n",
      "Perplexity: 4.03\n",
      "Next-token prediction probabilities:\n",
      "After 'Once' ‚Üí ' upon': 0.8850\n",
      "After ' upon' ‚Üí ' a': 0.9836\n",
      "After ' a' ‚Üí ' time': 0.9991\n",
      "After ' time' ‚Üí ',': 0.7619\n",
      "After ',' ‚Üí ' in': 0.0043\n",
      "After ' in' ‚Üí ' a': 0.9557\n",
      "After ' a' ‚Üí ' land': 0.0046\n",
      "After ' land' ‚Üí ' far': 0.1259\n",
      "After ' far' ‚Üí ' away': 0.7399\n",
      "After ' away' ‚Üí ',': 0.0409\n",
      "After ',' ‚Üí ' there': 0.9376\n",
      "After ' there' ‚Üí ' lived': 0.3566\n",
      "After ' lived' ‚Üí ' a': 0.9638\n",
      "After ' a' ‚Üí ' kind': 0.0211\n",
      "After ' kind' ‚Üí ' and': 0.0161\n",
      "After ' and' ‚Üí ' beautiful': 0.0090\n",
      "After ' beautiful' ‚Üí ' purple': 0.0015\n",
      "After ' purple' ‚Üí ' cat': 0.0291\n",
      "After ' cat' ‚Üí '.': 0.7238\n",
      "After '.' ‚Üí ' One': 0.2391\n",
      "After ' One' ‚Üí ' day': 0.9901\n",
      "After ' day' ‚Üí ',': 0.9839\n",
      "After ',' ‚Üí ' the': 0.6719\n",
      "After ' the' ‚Üí ' cat': 0.5654\n",
      "After ' cat' ‚Üí ' met': 0.1722\n",
      "After ' met' ‚Üí ' a': 0.9867\n",
      "After ' a' ‚Üí ' little': 0.4567\n",
      "After ' little' ‚Üí ' girl': 0.5944\n",
      "After ' girl' ‚Üí ' named': 0.8038\n",
      "After ' named' ‚Üí ' Sue': 0.1320\n",
      "After ' Sue' ‚Üí '.': 0.9816\n",
      "After '.' ‚Üí ' Sue': 0.9562\n",
      "After ' Sue' ‚Üí ' wanted': 0.1385\n",
      "After ' wanted' ‚Üí ' to': 0.9722\n",
      "After ' to' ‚Üí ' help': 0.5599\n",
      "After ' help' ‚Üí ' her': 0.2201\n",
      "After ' her' ‚Üí ' friend': 0.0957\n",
      "After ' friend' ‚Üí ',': 0.8207\n",
      "After ',' ‚Üí ' Tom': 0.1040\n",
      "After ' Tom' ‚Üí '.': 0.7707\n",
      "After '.' ‚Üí ' \"': 0.0309\n",
      "After ' \"' ‚Üí 'Let': 0.2219\n",
      "After 'Let' ‚Üí ''s': 0.9471\n",
      "After ''s' ‚Üí ' play': 0.1619\n",
      "After ' play' ‚Üí ' together': 0.1970\n",
      "After ' together' ‚Üí '!\"': 0.8390\n",
      "After '!\"' ‚Üí ' said': 0.5682\n",
      "After ' said' ‚Üí ' Sue': 0.5642\n",
      "After ' Sue' ‚Üí '.': 0.6090\n"
     ]
    }
   ],
   "source": [
    "# Run for each\n",
    "evaluate_and_print(greedy_output, \"Greedy\")\n",
    "evaluate_and_print(topk_output, \"Top-k\")\n",
    "evaluate_and_print(topp_output, \"Top-p\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPT1Layer",
   "language": "python",
   "name": "venv_tinystories"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
